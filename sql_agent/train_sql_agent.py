#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""SQL Agent è®­ç»ƒè„šæœ¬ï¼Œä½¿ç”¨ VERL ç®—æ³•ã€‚

ä½¿ç”¨ç¤ºä¾‹ï¼š

```bash
python train_sql_agent.py --train-file data/train.parquet --val-file data/val.parquet
```

ä½¿ç”¨å¤–éƒ¨å­˜å‚¨æ—¶ï¼Œå…ˆå¯åŠ¨å­˜å‚¨æœåŠ¡ï¼š

```bash
agl store --port 9999
```

ç„¶åä½¿ç”¨å¤–éƒ¨å­˜å‚¨åœ°å€è¿è¡Œè®­ç»ƒè„šæœ¬ï¼š

```bash
AGL_MANAGED_STORE=0 python train_sql_agent.py --external-store-address http://localhost:9999
```

ä¹Ÿå¯ä»¥åˆ†åˆ«è¿è¡Œç®—æ³•å’Œ runnerï¼š

```bash
AGL_MANAGED_STORE=0 AGL_CURRENT_ROLE=algorithm python train_sql_agent.py --external-store-address http://localhost:9999
AGL_MANAGED_STORE=0 AGL_CURRENT_ROLE=runner python train_sql_agent.py --external-store-address http://localhost:9999
```
"""

import argparse
import os
import dotenv
import uuid
from datetime import datetime
from typing import Any, Dict, Optional, cast

from sql_agent import SQLSearchAgent
from tools import query_database_by_sql
from datasets import Dataset as HuggingFaceDataset

import agentlightning as agl
from agentlightning.env_var import LightningEnvVar, resolve_bool_env_var, resolve_str_env_var
dotenv.load_dotenv()

def verl_default_config() -> Dict[str, Any]:
    config = {
        "algorithm": {
            "adv_estimator": "grpo",
            "use_kl_in_reward": False,
        },
        "data": {
            "train_batch_size": 2,
            "max_prompt_length": 2048,
            "max_response_length": 768,
        },
        "actor_rollout_ref": {
            "rollout": {
                "tensor_model_parallel_size": 1,
                "ulysses_sequence_parallel_size": 1,
                "n": 4,
                "log_prob_micro_batch_size_per_gpu": 1,
                "multi_turn": {"format": "hermes", "max_turns": 2},
                "name": "vllm",
                "gpu_memory_utilization": 0.35,
                "engine_kwargs": {
                    "vllm": {
                        "enable_auto_tool_choice": True,
                        "tool_call_parser": "hermes",
                    }
                },
            },
            "actor": {
                "ppo_mini_batch_size": 2,
                "ppo_micro_batch_size_per_gpu": 1,
                "optim": {"lr": 1e-6},
                "use_kl_loss": False,
                "kl_loss_coef": 0.0,
                "entropy_coeff": 0,
                "clip_ratio_low": 0.2,
                "clip_ratio_high": 0.3,
                "fsdp_config": {
                    "param_offload": True,
                    "optimizer_offload": True,
                },
            },
            "ref": {
                "log_prob_micro_batch_size_per_gpu": 1,
                "fsdp_config": {"param_offload": True},
            },
            "model": {
                "path": "Qwen/Qwen3-0.6B-Instruct",
                "use_remove_padding": False,
                "enable_gradient_checkpointing": True,
            },
        },
        "trainer": {
            "n_gpus_per_node": 1,  # ä¿®æ”¹ä¸ºä½ çš„ GPU æ•°é‡
            "val_before_train": True,
            "critic_warmup": 0,
            "logger": ["console", "wandb"],
            "project_name": "AgentLightning",
            "experiment_name": "sql_agent",
            "nnodes": 1,
            "save_freq": 20,
            "test_freq": 20,
            "total_epochs": 2,
        },
        "checkpoint": {
            "enable": True,  # å¯ç”¨æ£€æŸ¥ç‚¹åŠŸèƒ½
            "max_num_checkpoints": 2,  # æœ€å¤šä¿ç•™ 2 ä¸ªæ£€æŸ¥ç‚¹
        }
    }
    return config


def train(
    *,
    train_file: str,
    val_file: str,
    model: Optional[str],
    llm_proxy: bool,
    ci: bool,
    ci_fast: bool,
    n_runners: int,
    n_gpus: int,
    external_store_address: str,
    trajectory_level: bool = False,
    weave: bool,
    mongo_uri: Optional[str],
    resume_checkpoint: Optional[str] = None,
):
    """SQL Agent è®­ç»ƒå…¥å£å‡½æ•°ï¼Œä½¿ç”¨ VERL ç®—æ³•ã€‚

    å‚æ•°:
        train_file: è®­ç»ƒæ•°æ®é›†çš„ parquet æ–‡ä»¶è·¯å¾„ã€‚
        val_file: éªŒè¯æ•°æ®é›†çš„ parquet æ–‡ä»¶è·¯å¾„ã€‚
        model: HuggingFace æ¨¡å‹ ID æˆ–æœ¬åœ°è·¯å¾„ï¼Œç”¨äºè¦†ç›–é»˜è®¤æ¨¡å‹ã€‚
        llm_proxy: æ˜¯å¦å¯ç”¨ LLM Proxy è¿½è¸ª/é€‚é…å™¨ã€‚
        ci: æ˜¯å¦è¿è¡Œæœ€å°åŒ–çš„ CI é£æ ¼è®­ç»ƒå¾ªç¯ã€‚
        n_runners: Trainer çš„ runner æ•°é‡ã€‚
        ci_fast: æ˜¯å¦å°†è®­ç»ƒå¾ªç¯é™åˆ¶ä¸ºå•æ­¥æ‰§è¡Œï¼ˆéšå« --ciï¼‰ã€‚
        external_store_address: è¿æ¥å¤–éƒ¨å­˜å‚¨è€Œéåˆ›å»ºå†…å­˜å­˜å‚¨ã€‚
        trajectory_level: æ˜¯å¦åœ¨ trace aggregator ä¸­å¯ç”¨è½¨è¿¹çº§åˆ«ã€‚
        weave: æ˜¯å¦å¯ç”¨ Weave è¿½è¸ªã€‚
        mongo_uri: ç”¨äºå­˜å‚¨çš„ MongoDB URIã€‚
    """
    # åŠ è½½æ•°æ®é›†ï¼ˆéµå¾ª CLI æ–‡ä»¶è·¯å¾„ï¼‰
    train_dataset = cast(agl.Dataset[Dict[str, Any]], HuggingFaceDataset.from_parquet(train_file).to_list())  # type: ignore
    val_dataset = cast(agl.Dataset[Dict[str, Any]], HuggingFaceDataset.from_parquet(val_file).to_list())  # type: ignore

    print("è®­ç»ƒæ•°æ®é›†å‰ 5 è¡Œï¼š")
    print(train_dataset[:5])  # type: ignore
    print("éªŒè¯æ•°æ®é›†å‰ 5 è¡Œï¼š")
    print(val_dataset[:5])  # type: ignore

    config = verl_default_config()

    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é»˜è®¤ GPU é…ç½®
    if n_gpus > 1:
        config["trainer"]["n_gpus_per_node"] = n_gpus
        # è°ƒæ•´ batch size ä»¥é€‚åº”å¤šå¡è®­ç»ƒ
        # å¹¶è¡Œæ¨¡å‹è®¾ç½®
        config["actor_rollout_ref"]["rollout"]["tensor_model_parallel_size"] = n_gpus
        config["actor_rollout_ref"]["rollout"]["ulysses_sequence_parallel_size"] = n_gpus

        # å›ºå®šå¾®æ‰¹æ¬¡
        config["actor_rollout_ref"]["rollout"]["log_prob_micro_batch_size_per_gpu"] = 1
        config["actor_rollout_ref"]["actor"]["ppo_micro_batch_size_per_gpu"] = 1
        config["actor_rollout_ref"]["ref"]["log_prob_micro_batch_size_per_gpu"] = 1

        # CPU Offload on FSDP
        config["actor_rollout_ref"]["actor"]["fsdp_config"]["param_offload"] = True
        config["actor_rollout_ref"]["actor"]["fsdp_config"]["optimizer_offload"] = True

        # å¯é€‰ï¼šæŒ‰ GPU æ•°çº¿æ€§æ‰©å¢å…¨å±€ batch
        config["data"]["train_batch_size"] = 1 * n_gpus

        print(f"å¯ç”¨å¤šå¡è®­ç»ƒ: {n_gpus} GPUs, tensor_model_parallel_size={n_gpus}, ulysses_sequence_parallel_size={n_gpus}")
        print(f"Train batch per step = {config['data']['train_batch_size']}, micro-batch per GPU = 1")

    if model:
        config["actor_rollout_ref"]["model"]["path"] = model

    if resume_checkpoint is not None:
        if resume_checkpoint == "latest":
            print("ğŸ” ä»æœ€è¿‘ä¸€æ¬¡ checkpoint æ¢å¤è®­ç»ƒ")
            config["trainer"]["load_checkpoint"] = True
        else:
            print(f"ğŸ” ä»æŒ‡å®š checkpoint æ¢å¤è®­ç»ƒ: {resume_checkpoint}")
            config["trainer"]["load_checkpoint"] = resume_checkpoint

    if trajectory_level:
        config["agentlightning"] = {
            "trace_aggregator": {
                "level": "trajectory",
                "trajectory_max_prompt_length": 2048,
                "trajectory_max_response_length": 8192,
            }
        }
        print("å·²åœ¨ trace aggregator ä¸­å¯ç”¨è½¨è¿¹çº§åˆ«ã€‚")

    # CI å¼€å…³ä¿æŒå…¶ä»–è®¾ç½®ä¸å˜ï¼Œä½†å¯ä»¥è°ƒæ•´è½»é‡çº§å‚æ•°
    if ci or ci_fast:
        # é…ç½®å®éªŒåç§°å’Œé¡¹ç›®åç§°ï¼Œä»¥ä¾¿ CI ä½¿ç”¨
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        random_suffix = uuid.uuid4().hex[:8]
        EXPERIMENT_NAME = f"sql_agent_{timestamp}_{random_suffix}"

        PROJECT_NAME = "AgentLightningCI"

        # å¦‚æœ AGL_CURRENT_ROLE æ˜¯ runner åˆ™è·³è¿‡æ­¤æ­¥éª¤
        agl_current_role = resolve_str_env_var(LightningEnvVar.AGL_CURRENT_ROLE)

        if agl_current_role != "runner":
            # æ¨¡æ‹Ÿå†™å…¥ $GITHUB_OUTPUTï¼ˆå¦‚æœè®¾ç½®äº†çš„è¯ï¼‰
            github_output = os.getenv("GITHUB_OUTPUT")
            if github_output:
                with open(github_output, "a") as f:
                    f.write(f"project_name={PROJECT_NAME}\n")
                    f.write(f"run_name={EXPERIMENT_NAME}\n")

            print("è®¾ç½®ç¯å¢ƒå˜é‡ï¼š")
            print(f"PROJECT_NAME={PROJECT_NAME}")
            print(f"EXPERIMENT_NAME={EXPERIMENT_NAME}")

        # ä¿æŒè½»é‡çº§ï¼Œä¸æ·»åŠ æ–°å‚æ•°
        config["actor_rollout_ref"]["rollout"]["gpu_memory_utilization"] = 0.3
        config["trainer"]["total_epochs"] = 1
        config["trainer"]["total_training_steps"] = 20
        config["trainer"]["test_freq"] = 20
        config["trainer"]["experiment_name"] = EXPERIMENT_NAME
        config["trainer"]["project_name"] = PROJECT_NAME
        config["trainer"].pop("save_freq", None)

        if ci_fast:
            # ç”¨äºæµ‹è¯•çš„è¶…å¿« CI å¼€å…³
            config["actor_rollout_ref"]["rollout"]["gpu_memory_utilization"] = 0.3
            config["trainer"]["total_training_steps"] = 1
            config["trainer"]["test_freq"] = 1

    algorithm = agl.VERL(config)

    if external_store_address:
        store: Optional[agl.LightningStore] = agl.LightningStoreClient(external_store_address)
    elif mongo_uri:
        from agentlightning.store.mongo import MongoLightningStore

        store = MongoLightningStore(mongo_uri=mongo_uri)
    else:
        store = None

    if llm_proxy:
        tracer = agl.OtelTracer()  # LLM Proxy çš„è™šæ‹Ÿ tracer
        adapter = agl.LlmProxyTraceToTriplet()
        trainer = agl.Trainer(algorithm=algorithm, n_runners=n_runners, store=store, tracer=tracer, adapter=adapter)
    elif weave:
        # åº”å§‹ç»ˆå»¶è¿Ÿ/æŒ‰æ¡ä»¶å¯¼å…¥ï¼ˆåœ¨åŠŸèƒ½æ ‡å¿—åé¢ï¼‰ï¼Œä»¥é¿å…åœ¨æœªæ˜¾å¼å¯ç”¨ weave æ—¶å¹²æ‰°
        # å…¶ä»–åº“ï¼ˆå¦‚ LiteLLM/OpenTelemetryï¼‰ã€‚
        from agentlightning.tracer.weave import WeaveTracer

        tracer = WeaveTracer()
        trainer = agl.Trainer(algorithm=algorithm, n_runners=n_runners, store=store, tracer=tracer)
    else:
        print(f"ä¸è¿›è¡Œè®­ç»ƒï¼Œå› ä¸ºæ²¡æœ‰å¼€å¯ --llm-proxy æˆ– --weave é€‰é¡¹ã€‚")
        trainer = agl.Trainer(algorithm=algorithm, n_runners=n_runners, store=store)

    trainer.fit(SQLSearchAgent(), train_dataset, val_dataset=val_dataset)


def main():
    # è®¾ç½® CUDA å†…å­˜é…ç½®ï¼Œé¿å…ç¢ç‰‡åŒ–å¯¼è‡´ OOM
    os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")

    parser = argparse.ArgumentParser(description="ä½¿ç”¨ Agent-lightning + VERL è®­ç»ƒ SQL Agentã€‚")
    parser.add_argument("--train-file", type=str, default="data/train.parquet", help="è®­ç»ƒæ•°æ®é›†çš„ parquet æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--val-file", type=str, default="data/val.parquet", help="éªŒè¯æ•°æ®é›†çš„ parquet æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", type=str, default=None, help="HuggingFace æ¨¡å‹ ID æˆ–æœ¬åœ°è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--llm-proxy", action="store_true", help="å¯ç”¨ LLM Proxy è¿½è¸ª/é€‚é…å™¨")
    parser.add_argument("--weave", action="store_true", help="å¯ç”¨ Weave è¿½è¸ª")
    parser.add_argument("--ci", action="store_true", help="è¿è¡Œæœ€å°åŒ–çš„ CI é£æ ¼è®­ç»ƒå¾ªç¯")
    parser.add_argument(
        "--ci-fast", action="store_true", help="å°†è®­ç»ƒå¾ªç¯é™åˆ¶ä¸ºå•æ­¥æ‰§è¡Œï¼ˆéšå« --ciï¼‰"
    )
    parser.add_argument("--n-runners", type=int, default=4, help="Trainer çš„ runner workeræ•°é‡")
    parser.add_argument("--n-gpus", type=int, default=1, help="æ¯ä¸ªèŠ‚ç‚¹çš„ GPU æ•°é‡")
    parser.add_argument(
        "--external-store-address",
        type=str,
        default="",
        help="è¿æ¥å¤–éƒ¨å­˜å‚¨è€Œéåˆ›å»ºå†…å­˜å­˜å‚¨",
    )
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ—¥å¿—")
    parser.add_argument(
        "--trajectory-level",
        action="store_true",
        help="åœ¨ trace aggregator ä¸­å¯ç”¨è½¨è¿¹çº§åˆ«ã€‚",
    )
    parser.add_argument(
        "--mongo-uri",
        type=str,
        default=None,
        help="ç”¨äºå­˜å‚¨çš„ MongoDB URIã€‚",
    )
    parser.add_argument(
        "--resume_checkpoint",
        type=str,
        default=None,
        nargs="?",
        const="latest",
        help=(
            "ä» checkpoint æ¢å¤è®­ç»ƒã€‚"
            "ä¸ä¼ å€¼è¡¨ç¤ºä»æœ€è¿‘ä¸€æ¬¡ checkpoint æ¢å¤ï¼Œ"
            "ä¼ è·¯å¾„è¡¨ç¤ºä»æŒ‡å®š checkpoint æ¢å¤ã€‚"
        ),
    )

    args = parser.parse_args()

    if args.external_store_address:
        print(f"æ­£åœ¨è¿æ¥åˆ°å¤–éƒ¨å­˜å‚¨ï¼š{args.external_store_address}")
        if resolve_bool_env_var(LightningEnvVar.AGL_MANAGED_STORE, fallback=True):
            raise ValueError(
                "ä½¿ç”¨å¤–éƒ¨å­˜å‚¨æ—¶ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ AGL_MANAGED_STORE=0ã€‚"
                "å¦åˆ™ Trainer ä»ä¼šå°è¯•ç®¡ç†å­˜å‚¨çš„ç”Ÿå‘½å‘¨æœŸï¼"
            )

    if args.ci_fast:
        args.ci = True

    agl.setup_logging("DEBUG" if args.debug else "INFO")

    train(
        train_file=args.train_file,
        val_file=args.val_file,
        model=args.model,
        llm_proxy=args.llm_proxy,
        ci=args.ci,
        ci_fast=args.ci_fast,
        n_runners=args.n_runners,
        n_gpus=args.n_gpus,
        external_store_address=args.external_store_address,
        trajectory_level=args.trajectory_level,
        weave=args.weave,
        mongo_uri=args.mongo_uri,
        resume_checkpoint=args.resume_checkpoint
    )


if __name__ == "__main__":
    main()
